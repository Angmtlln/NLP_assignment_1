{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIgM6C9HYUhm"
   },
   "source": [
    "# Context-sensitive Spelling Correction\n",
    "\n",
    "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
    "\n",
    "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
    "\n",
    "Useful links:\n",
    "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
    "- [Norvig's dataset](https://norvig.com/big.txt)\n",
    "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
    "\n",
    "Grading:\n",
    "- 60 points - Implement spelling correction\n",
    "- 20 points - Justify your decisions\n",
    "- 20 points - Evaluate on a test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-vb8yFOGRDF"
   },
   "source": [
    "## Implement context-sensitive spelling correction\n",
    "\n",
    "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
    "\n",
    "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
    "\n",
    "When solving this task, we expect you'll face (and successfully deal with) some problems or make up the ideas of the model improvement. Some of them are: \n",
    "\n",
    "- solving a problem of n-grams frequencies storing for a large corpus;\n",
    "- taking into account keyboard layout and associated misspellings;\n",
    "- efficiency improvement to make the solution faster;\n",
    "- ...\n",
    "\n",
    "Please don't forget to describe such cases, and what you decided to do with them, in the Justification section.\n",
    "\n",
    "##### IMPORTANT:  \n",
    "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
    "- Your implementation\n",
    "- Analysis of why the implemented approach is suggested\n",
    "- Improvements of the original approach that you have chosen to implement"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Заметки\n",
    "\n",
    "# bigrams.txt\n",
    "# Частота | Слово 1 | Слово 2\n",
    "#\n",
    "# 275  a    a\n",
    "# 29   a    all\n",
    "\n",
    "# coca_all_links\n",
    "# Частота | Слово 1 | Слово 2 | Часть речи 1 | Часть речи 2\n",
    "#\n",
    "# 36  a-National  Rank    jj   nn1\n",
    "# 92  abandoned   building  jj   nn1\n",
    "\n",
    "# fivegrams.txt\n",
    "# Частота | Слово 1 | Слово 2 | Слово 3 | Слово 4 | Слово 5\n",
    "#\n",
    "# 16  a    babe    in    the    woods\n",
    "# 6   a    baby    at    her    breast\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MoQeEsZvHvvi",
    "ExecuteTime": {
     "end_time": "2025-02-17T20:18:20.474893Z",
     "start_time": "2025-02-17T20:18:19.500894Z"
    }
   },
   "source": [
    "#Test bigrams\n",
    "import nltk\n",
    "\n",
    "nltk.download('words',quiet=True)\n",
    "nltk.download('reuters',quiet=True)\n",
    "nltk.download('punkt_tab',quiet=True)\n",
    "\n",
    "\n",
    "def generate_candidates(word):\n",
    "    abc = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    edits = set()\n",
    "\n",
    "    #Delite char\n",
    "    for i in range(len(word)):\n",
    "        edits.add(word[:i] + word[i+1:])\n",
    "\n",
    "    #Substitute char\n",
    "    for i in range(len(word)):\n",
    "        for char in abc:\n",
    "            edits.add(word[:i] + char + word[i+1:])\n",
    "\n",
    "     #Insert char\n",
    "    for i in range(len(word) + 1):\n",
    "        for char in abc:\n",
    "            edits.add(word[:i] + char + word[i:])\n",
    "    return edits & set(words.words())\n",
    "\n",
    "\n",
    "def correct_sentence_bigram(sentence, bigram_freq):\n",
    "    tokens = nltk.word_tokenize(sentence.lower())\n",
    "    corrected_tokens = []\n",
    "\n",
    "    for i in range(len(tokens)):\n",
    "        word = tokens[i]\n",
    "        if i > 0:\n",
    "            prev_word = corrected_tokens[-1]\n",
    "            candidates = generate_candidates(word)\n",
    "            if candidates:\n",
    "                best_candidate = max(candidates, key=lambda w: bigram_freq.get((prev_word, w), 1))\n",
    "                corrected_tokens.append(best_candidate)\n",
    "            else:\n",
    "                corrected_tokens.append(word)\n",
    "        else:\n",
    "            corrected_tokens.append(word)\n",
    "\n",
    "    return ' '.join(corrected_tokens)\n",
    "\n",
    "bigram_freq = {}\n",
    "with open(\"bigrams (2).txt\", 'r', encoding='latin-1') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split('\\t')\n",
    "        if len(parts) == 3:\n",
    "            freq, w1, w2 = parts\n",
    "            bigram_freq[(w1, w2)] = int(freq)\n",
    "\n",
    "\n",
    "sample_text = \"I am dking sport every day\"\n",
    "corrected_text = correct_sentence_bigram(sample_text, bigram_freq)\n",
    "print(corrected_text)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am doing spor revery dag\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T20:29:56.171401Z",
     "start_time": "2025-02-17T20:29:55.976396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Test coca\n",
    "import nltk\n",
    "from nltk.corpus import reuters, words\n",
    "\n",
    "\n",
    "nltk.download('words',quiet=True)\n",
    "nltk.download('reuters',quiet=True)\n",
    "nltk.download('punkt_tab',quiet=True)\n",
    "\n",
    "word_list = set(words.words())\n",
    "\n",
    "# Function to generate candidate corrections\n",
    "def generate_candidates(word):\n",
    "    abc = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    edits = set()\n",
    "\n",
    "   #Delite char\n",
    "    for i in range(len(word)):\n",
    "        edits.add(word[:i] + word[i+1:])\n",
    "\n",
    "    #Substitute char\n",
    "    for i in range(len(word)):\n",
    "        for char in abc:\n",
    "            edits.add(word[:i] + char + word[i+1:])\n",
    "\n",
    "     #Insert char\n",
    "    for i in range(len(word) + 1):\n",
    "        for char in abc:\n",
    "            edits.add(word[:i] + char + word[i:])\n",
    "\n",
    "    # Оставляем только реальные слова\n",
    "    valid_edits = {w for w in edits if w in word_list}\n",
    "    return valid_edits if valid_edits else {word}  # Если нет валидных, оставляем оригинал\n",
    "\n",
    "\n",
    "def correct_sentence_coca(sentence, coca_freq):\n",
    "    tokens = nltk.word_tokenize(sentence.lower())\n",
    "    corrected_tokens = []\n",
    "    for i in range(len(tokens)):\n",
    "        word = tokens[i]\n",
    "        candidates = generate_candidates(word)\n",
    "        if i > 0:\n",
    "            prev_word = corrected_tokens[-1]\n",
    "            best_candidate = max(candidates, key=lambda w: coca_freq.get((prev_word, w), 1), default=word)\n",
    "        else:\n",
    "            best_candidate = max(candidates, key=lambda w: w in word_list, default=word)\n",
    "        corrected_tokens.append(best_candidate)\n",
    "    return ' '.join(corrected_tokens)\n",
    "\n",
    "\n",
    "coca_freq = {}\n",
    "with open(\"coca_all_links (2).txt\", 'r', encoding='latin-1') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split('\\t')\n",
    "        if len(parts) >= 3:\n",
    "            freq, w1, w2 = parts[:3]\n",
    "            coca_freq[(w1, w2)] = int(freq)\n",
    "\n",
    "sample_text = \"I am dking sport every day\"\n",
    "corrected_text = correct_sentence_coca(sample_text, coca_freq)\n",
    "print(corrected_text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c om eking spor revery dag\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T20:34:24.410251Z",
     "start_time": "2025-02-17T20:34:17.878669Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Test fivegrams\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "from nltk.metrics import edit_distance\n",
    "\n",
    "# Скачать необходимые данные без вывода сообщений\n",
    "nltk.download('words', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "word_list = set(words.words())\n",
    "\n",
    "def correct_word(word):\n",
    "    if word in word_list:\n",
    "        return word  # Если слово уже правильное, оставляем его\n",
    "    closest_word = min(word_list, key=lambda w: edit_distance(word, w))\n",
    "    return closest_word\n",
    "\n",
    "def correct_sentence_fivegram(sentence, fivegram_freq):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    corrected_tokens = []\n",
    "\n",
    "    for i, word in enumerate(tokens):\n",
    "        word_lower = word.lower()\n",
    "        original_case = word[0].isupper() if word else False  # Проверяем заглавную букву\n",
    "\n",
    "        # Собираем последние 4 исправленных слова (или меньше, если в начале предложения)\n",
    "        context = tuple(corrected_tokens[max(0, i - 4):i])\n",
    "\n",
    "        if context in fivegram_freq:\n",
    "            candidates = fivegram_freq[context]\n",
    "            best_candidate = max(candidates, key=candidates.get)  # Выбираем слово с макс. частотой\n",
    "        else:\n",
    "            best_candidate = correct_word(word_lower)  # Если контекста нет, fallback на словарь\n",
    "        corrected_tokens.append(best_candidate.capitalize() if original_case else best_candidate)\n",
    "\n",
    "    return ' '.join(corrected_tokens)\n",
    "\n",
    "fivegram_freq = {}\n",
    "with open(\"fivegrams (2).txt\", 'r', encoding='latin-1') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split('\\t')\n",
    "        if len(parts) == 6:\n",
    "            freq, *words = parts\n",
    "            freq = int(freq)\n",
    "            context, target_word = tuple(words[:-1]), words[-1]  # Первые 4 слова - контекст, последнее - исправляемое слово\n",
    "            if context not in fivegram_freq:\n",
    "                fivegram_freq[context] = {}\n",
    "            fivegram_freq[context][target_word] = freq\n",
    "\n",
    "sample_text = \"I am dking sport every day\"\n",
    "corrected_text = correct_sentence_fivegram(sample_text, fivegram_freq)\n",
    "print(corrected_text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am ding sport every day\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T20:47:54.102147Z",
     "start_time": "2025-02-17T20:47:52.111668Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('reuters', quiet=True)\n",
    "nltk.download('words', quiet=True)\n",
    "\n",
    "word_list = nltk.corpus.reuters.words()  # Используем корпус новостей\n",
    "word_freq = Counter(word_list)\n",
    "\n",
    "# Сохраняем в файл\n",
    "with open(\"unigrams.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for word, freq in word_freq.most_common():\n",
    "        f.write(f\"{freq}\\t{word.lower()}\\n\")\n"
   ],
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T21:21:30.559338Z",
     "start_time": "2025-02-17T21:21:29.912385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import words\n",
    "\n",
    "nltk.download('words', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "word_list = set(words.words())\n",
    "\n",
    "# Загрузка частот слов P(w)\n",
    "def load_unigrams(filename):\n",
    "    word_freq = defaultdict(int)\n",
    "    total_count = 0\n",
    "    with open(filename, 'r', encoding='latin-1') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 2:\n",
    "                freq, word = parts\n",
    "                word_freq[word] = int(freq)\n",
    "                total_count += int(freq)\n",
    "    return word_freq, total_count\n",
    "\n",
    "# Загрузка биграмм P(w2 | w1)\n",
    "def load_bigrams(filename):\n",
    "    bigram_freq = defaultdict(int)\n",
    "    with open(filename, 'r', encoding='latin-1') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 3:\n",
    "                freq, w1, w2 = parts\n",
    "                bigram_freq[(w1, w2)] = int(freq)\n",
    "    return bigram_freq\n",
    "\n",
    "# Загружаем частоты\n",
    "unigram_freq, total_unigrams = load_unigrams(\"unigrams.txt\")\n",
    "bigram_freq = load_bigrams(\"bigrams (2).txt\")\n",
    "\n",
    "# Вероятность P(w)\n",
    "def P(word):\n",
    "    return unigram_freq[word] / total_unigrams if word in unigram_freq else 1e-6\n",
    "\n",
    "# Вероятность P(w2 | w1)\n",
    "def P_bigram(w1, w2):\n",
    "    return bigram_freq.get((w1, w2), 1) / unigram_freq.get(w1, 1)\n",
    "\n",
    "# Генерация исправлений (опечатки)\n",
    "def edits1(word):\n",
    "    abc = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    deletes = [L + R[1:] for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
    "    replaces = [L + c + R[1:] for L, R in splits if R for c in abc]\n",
    "    inserts = [L + c + R for L, R in splits for c in abc]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def known(words):\n",
    "    return {w for w in words if w in unigram_freq}  # Только слова из словаря\n",
    "\n",
    "# Исправление слова\n",
    "def correct_word(word, prev_word):\n",
    "    candidates = known([word]) or known(edits1(word)) or {word}\n",
    "    return max(candidates, key=lambda w: P_bigram(prev_word, w) * P(w))\n",
    "\n",
    "# Исправление предложения\n",
    "def correct_sentence(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence.lower())\n",
    "    corrected_tokens = []\n",
    "\n",
    "    for i, word in enumerate(tokens):\n",
    "        if i == 0:\n",
    "            corrected_tokens.append(word)  # Первое слово без исправления\n",
    "        else:\n",
    "            corrected_tokens.append(correct_word(word, corrected_tokens[-1]))\n",
    "\n",
    "    return ' '.join(corrected_tokens)\n",
    "\n",
    "sample_text = \"I jsut wnat to say hello.\"\n",
    "corrected_text = correct_sentence(sample_text)\n",
    "print(corrected_text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i just what to say hell .\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oML-5sJwGRLE"
   },
   "source": [
    "## Justify your decisions\n",
    "\n",
    "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
    "- Which ngram dataset to use\n",
    "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
    "- Beam search parameters\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Xb_twOmVsC6"
   },
   "source": [
    "*Your text here...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46rk65S4GRSe"
   },
   "source": [
    "## Evaluate on a test set\n",
    "\n",
    "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity (or just take another dataset). Compare your solution to the Norvig's corrector, and report the accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OwZWaX9VVs7B"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Useful resources (also included in the archive in moodle):\n",
    "\n",
    "1. [Possible dataset with N-grams](https://www.ngrams.info/download_coca.asp)\n",
    "2. [Damerau–Levenshtein distance](https://en.wikipedia.org/wiki/Damerau–Levenshtein_distance#:~:text=Informally%2C%20the%20Damerau–Levenshtein%20distance,one%20word%20into%20the%20other.)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
